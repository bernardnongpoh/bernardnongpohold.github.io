[{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Bernard Nongpoh","type":"authors"},{"authors":null,"categories":["Miscellaneous","Jekyll"],"content":" Reverie is a Jekyll-powered theme which is simple and opinionated. It\u0026rsquo;s actually a fork of jekyll-now with some additional features and personal touches which I\u0026rsquo;ve implemented to suit my needs for my blog.\nThis is a plug-and-play Jekyll theme which you can use on GitHub Pages without even setting up a local environment.\nFeatures  Command-line free fork-first workflow, using GitHub.com to create, customize and post to your blog Fully responsive and mobile optimized base theme (Theme Demo) Sass/Coffeescript support using Jekyll 2.0 Free hosting on your GitHub Pages user site Markdown blogging Elegant typography  Futura PT fonts (The same fonts which has been used on https://pixar.com)  Syntax highlighting using Pygments  Dracula syntax theme included  Disqus commenting Google Analytics integration Categorize posts out-of-the box A home widget to show recent GitHub commit RSS Feed  Using Reverie on GitHub Pages Step 1) Fork Reverie to your User Repository Fork this repo, then rename the repository to yourgithubusername.github.io.\nYour Jekyll blog will often be viewable immediately at https://yourgithubusername.github.io (if it\u0026rsquo;s not, you can often force it to build by completing step 2)\nStep 2) Customize and view your site Enter your site name, description, avatar and many other options by editing the _config.yml file. You can easily turn on Google Analytics tracking, Disqus commenting and social icons here.\nMaking a change to _config.yml (or any file in your repository) will force GitHub Pages to rebuild your site with jekyll. Your rebuilt site will be viewable a few seconds later at https://yourgithubusername.github.io - if not, give it ten minutes as GitHub suggests and it\u0026rsquo;ll appear soon.\nStep 3) Publish your first blog post Create a new file called /_posts/2019-2-13-Hello-World.md to publish your first blog post. That\u0026rsquo;s all you need to do to publish your first blog post! This Markdown Cheatsheet might come in handy while writing the posts.\n You can add additional posts in the browser on GitHub.com too! Just hit the Create new file button in /_posts/ to create new content. Just make sure to include the front-matter block at the top of each new blog post and make sure the post\u0026rsquo;s filename is in this format: year-month-day-title.md\n Using Categories in Reverie You can categorize your content based on categories in Reverie. For this, you just need to add categories in front matter like below:\nFor adding single category:\ncategories: JavaScript  For adding multiple categories:\ncategories: [PHP, Laravel]  The contegorized content can be shown over this URL: https://yourgithubusername.github.io/categories/\nRSS The generated RSS feed of your blog can be found at https://yourgithubusername.github.io/feed. You can see the example RSS feed over here.\nSitemap The generated sitemap of your blog can be found at https://yourgithubusername.github.io/sitemap.\nLicense MIT\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7dd70408625e5cf23f4b5e508acf24a6","permalink":"/post/2019-02-13-introducing-reverie-jekyll-theme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2019-02-13-introducing-reverie-jekyll-theme/","section":"post","summary":"Reverie is a Jekyll-powered theme which is simple and opinionated. It\u0026rsquo;s actually a fork of jekyll-now with some additional features and personal touches which I\u0026rsquo;ve implemented to suit my needs for my blog.\nThis is a plug-and-play Jekyll theme which you can use on GitHub Pages without even setting up a local environment.\nFeatures  Command-line free fork-first workflow, using GitHub.com to create, customize and post to your blog Fully responsive and mobile optimized base theme (Theme Demo) Sass/Coffeescript support using Jekyll 2.","tags":null,"title":"Introducing Reverie - A ridiculously elegant Jekyll theme","type":"post"},{"authors":null,"categories":["Neural Networks and Deep Learning"],"content":" Week 1 Summary Welcome In this video Andrew Ng. walkthrough the deep learning specialization which includes Neural Networks and deep learning, hyperparameter tuning, structural machine learning projects, CNN and finally Recurrent Neural Network. The quote by Andrew Ng. that is AI is the new Electricity. This quote is really a powerful words that has already shown a disruptive of using AI across industries just like the electrification does in the Industrial Era. The main motive in this specialization is to be able to build and tunes deep learning model and apply to various problems across industries.\nWhat is Neural Network? Neural network is basically implements a function that maps a series of input to output. Let us consider a simple housing price prediction problem, that given the size of the house (in square feet), the job of this prediction is to predicts the price.\nThe line that is shown in the figure is a ReLU (Rectified Linear Unit) function. Neural Network is a stacking of these small neuron.\nIn the figure below, we have three properties or features of the housing. It can be the size, number of bedrooms, the postal code and maybe wealth. Taking the size and number of bedrooms we can predicts the family size. Also taking the Zip code, we can map the walkability distance and finally if we take postal code and wealth we can predicts the quality of the school in that area. This layers can be assume to be predicted by the neural network and let neural network decides what to do with it. We call this kind of layer as hidden layers. The goal of neural network is to takes input and predict the output or an approximation of input to output. The layer that is not the input and output are called the hidden layer.\nSupervised Learning with Neural Networks one type of economical value by ML is supervised ML. Cleverly choosen of X and Y.\nNeural Network Example - Standard NN - Convolutional NN - Recurrent NN\nThere are two types of data: Structure data and unstructure data.\nWhy deeplearning taking off? Changes from Sigmoid to RELU due to gradient. Convergent is fast. Gradient and decent works faster. - Data -Algorithm -Computation\nIteration in DL Idea -\u0026gt; Code -\u0026gt; Experiment\nCourse Resources deeplearning.ai\nfine representation pixel\u0026ndash;\u0026gt; co-ordinate\u0026ndash;\u0026gt;matrix multiply. Generic principle. Represent from one to linear representation.\nRead the literature and look for something wrong and then work on it.\nTrust your intuition. Go for it. Don\u0026rsquo;t worry if every body says it is nonsense.\nWhen you\u0026rsquo;re thinking you have a very good idea and somebody says that its nonsense or nothing. Then you\u0026rsquo;re really into something.\nGefrey Hinton Reading List\nDiscussion Forum Why is learning slower in a sigmoid function? Oliver Philip\nIn the context of deep learning, vanishing gradient is the main issue with sigmoid activation since multiplying a number (each element of gradient) by a value between 0 and 1 many times (DL = many layers) will rapidly lead to a quasi null value.\nReLU activation takes care of that issue for any positive value but what about negative value?\nThe answer simply is: we do not need negative value.\nA single neuron can be seen as a detector with all entry associated with a positive weight contributing to the detection and all entry associated with a negative weight diminishing the detection. If the neuron weighted value before activation is negative, there is no detection.\nSo with ReLU activation, we just don\u0026rsquo;t use the \u0026ldquo;non detection\u0026rdquo; part.\nBut what if the network need the \u0026ldquo;non detection\u0026rdquo; part?\nThen another neuron with the exact same value weights but with all signs inverted will be used for this new detection task.\nGiovanni Bianchi\nI would like to tack on Bhavul\u0026rsquo;s excellent answer with some more high-level and qualitative considerations that might give you some more context.\nI interpret Andrew\u0026rsquo;s remark as having to do with the slope of the function at the ends. The sigmoid becomes flat and then inherently \u0026ldquo;stable\u0026rdquo;, meaning the output doesn\u0026rsquo;t react very much to variations of the input, making learning slow once certain input values are reached.\nThe ReLU function, instead, is linear for x \u0026gt; 0 in the impact of input over output, resulting in a much more responsive system, one that is quicker to learn.\nI\u0026rsquo;m sure we\u0026rsquo;ll see in the following lessons, that the tail stability of the Sigmoid will turn out not to be desirable in some situations.\nEd Mitby Mentor\nInteresting, here are the results I am getting:\nsigmoid(x): 1.5 ms\nsigmoid_derivative(x): 205 ns\nRELU(x): 137 ns\nSWISH(x): 1.82 ms\n$\\sqrt{3x-1}+(1+x)^2$\nAt first glance it appears the \u0026ldquo;return 1 if x\u0026gt;0 else 0\u0026rdquo; and \u0026ldquo;x*(1-x) \u0026ldquo; are much faster than the numpy exp function.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"943d85c0fe9573c7e27b0e5736ed7ecc","permalink":"/post/2019-06-20-neural-networks-deep-learning-week1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2019-06-20-neural-networks-deep-learning-week1/","section":"post","summary":"Week 1 Summary Welcome In this video Andrew Ng. walkthrough the deep learning specialization which includes Neural Networks and deep learning, hyperparameter tuning, structural machine learning projects, CNN and finally Recurrent Neural Network. The quote by Andrew Ng. that is AI is the new Electricity. This quote is really a powerful words that has already shown a disruptive of using AI across industries just like the electrification does in the Industrial Era.","tags":null,"title":"Neural Networks and Deep Learning - Week 1","type":"post"},{"authors":null,"categories":["Neural Networks and Deep Learning"],"content":" Week 2 : Logistic Regression as a Neural Network : Summary Binary Classification Notation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"467359c9ed2ad3609b7f75883f7c54ff","permalink":"/post/2019-06-21-neural-networks-deep-learning-week2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2019-06-21-neural-networks-deep-learning-week2/","section":"post","summary":"Week 2 : Logistic Regression as a Neural Network : Summary Binary Classification Notation","tags":null,"title":"Neural Networks and Deep Learning - Week 2","type":"post"},{"authors":null,"categories":["Fast.ai Lesson 1"],"content":" Practical Deep Learning for Coders, V3 Lesson 1 by Jeremy Howard Cloud Computing for Deep Learning  Google compute platform Salamander Colab - free  I will be using Colab for this course. No money to pay for cloud https://course.fast.ai/start_colab.html\nIn Google Colab we need to save our work locally, meaning that the Google Clab is not responsible for saving our work.\nThe first step is to download the course material from fast.ai to do that, go ahead add a link to the github repository as shown in Figure below.\nTo change the runtime of the Colab. Go to Runtime and click change runtime type and select CPU/CPU/TPU (Tensor Processing Unit)\nConfigure the notebook instance Install necessary packages as below by creating a code cell, and running:\n!curl -s https://course.fast.ai/setup/colab | bash\nSaving the data files to Google drive\nWe let Google Colab autorize accesses to drive.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bdfdd04e2d3fe9bdc993491099c3617f","permalink":"/post/2019-06-29-practical-deep-learning-for-coders-getting-started/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2019-06-29-practical-deep-learning-for-coders-getting-started/","section":"post","summary":"Practical Deep Learning for Coders, V3 Lesson 1 by Jeremy Howard Cloud Computing for Deep Learning  Google compute platform Salamander Colab - free  I will be using Colab for this course. No money to pay for cloud https://course.fast.ai/start_colab.html\nIn Google Colab we need to save our work locally, meaning that the Google Clab is not responsible for saving our work.\nThe first step is to download the course material from fast.","tags":null,"title":"Neural Networks and Deep Learning - Week 2","type":"post"},{"authors":null,"categories":["Fast.ai Lesson 1"],"content":" Practical Deep Learning for Coders, V3 Lesson 1 Video by Jeremy Howard Code section\nfast Ai sit at the top of Pytorch\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd8579e375c9fffc31fc7bbdde3f34c4","permalink":"/post/2019-06-29-practical-deep-learning-for-coders-lesson-1-video/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2019-06-29-practical-deep-learning-for-coders-lesson-1-video/","section":"post","summary":"Practical Deep Learning for Coders, V3 Lesson 1 Video by Jeremy Howard Code section\nfast Ai sit at the top of Pytorch","tags":null,"title":"Neural Networks and Deep Learning - Week 2","type":"post"}]